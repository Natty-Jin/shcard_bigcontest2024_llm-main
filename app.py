import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss

import streamlit as st

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ê²½ë¡œ ì„¤ì •
data_path = "./data"
module_path = "./modules"

# Gemini ì„¤ì •
import google.generativeai as genai

GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
genai.configure(api_key=GOOGLE_API_KEY)

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash")

# CSV íŒŒì¼ ë¡œë“œ
csv_file_path = "JEJU_MCT_DATA_modified.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# ìµœì‹  ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df["ê¸°ì¤€ì—°ì›”"] == df["ê¸°ì¤€ì—°ì›”"].max()].reset_index(drop=True)

# Streamlit App UI ì„¤ì •
st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

with st.sidebar:
    st.title("ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘")
    st.write("")

    st.subheader("ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?")
    time = st.selectbox("ì‹œê°„ëŒ€ ì„ íƒ", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time", label_visibility="hidden")
    st.write("")

    st.subheader("ì–´ë“œë ˆê°€ ë§˜ì— ë“œì‹ ë””ê°€?")
    local_choice = st.radio("ë§›ì§‘ ì„ íƒ", ("ì œì£¼ë„ë¯¼ ë§›ì§‘", "ê´€ê´‘ê° ë§›ì§‘"), label_visibility="hidden")
    st.write("")

st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")
st.write("")
st.write("#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤")
st.write("")

image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)
st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}
    ]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}
    ]

st.sidebar.button("Clear Chat History", on_click=clear_chat_history)

# RAG ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

print(f"Device is {device}.")

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, "faiss_index.index")):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        print(f"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”©
def embed_text(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, "embeddings_array_file.npy"))

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response_with_faiss(
    question, df, embeddings, model, embed_text, time, local_choice,
    index_path=os.path.join(module_path, "faiss_index.index"), max_count=10, k=3, print_prompt=True
):
    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    index = load_faiss_index(index_path)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question).reshape(1, -1)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)
    distances, indices = index.search(query_embedding, k * 3)

    # ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ
    filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True)

    # ì‹œê°„ëŒ€ í•„í„°ë§
    if time == "ì•„ì¹¨":
        filtered_df = filtered_df[filtered_df["ì˜ì—…ì‹œê°„"].apply(lambda x: any(hour in eval(x) for hour in range(5, 12)))]
    elif time == "ì ì‹¬":
        filtered_df = filtered_df[filtered_df["ì˜ì—…ì‹œê°„"].apply(lambda x: any(hour in eval(x) for hour in range(12, 14)))]
    elif time == "ì˜¤í›„":
        filtered_df = filtered_df[filtered_df["ì˜ì—…ì‹œê°„"].apply(lambda x: any(hour in eval(x) for hour in range(14, 18)))]
    elif time == "ì €ë…":
        filtered_df = filtered_df[filtered_df["ì˜ì—…ì‹œê°„"].apply(lambda x: any(hour in eval(x) for hour in range(18, 23)))]
    elif time == "ë°¤":
        filtered_df = filtered_df[filtered_df["ì˜ì—…ì‹œê°„"].apply(lambda x: any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))]

    if filtered_df.empty:
        return f"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.head(k)

    # í˜„ì§€ì¸ ë§›ì§‘ ì˜µì…˜ ë°˜ì˜
    local_choice = "ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘" if local_choice == "ì œì£¼ë„ë¯¼ ë§›ì§‘" else "í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘"

    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    reference_info = "\n".join(filtered_df["text"])
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ìµœëŒ€ í† í° ìˆ˜ì— ëŒ€í•œ íŒíŠ¸ ì¶”ê°€
    # system_message = "Max Tokenì€ 5000ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.\n"
    # prompt = f"{system_message}ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"
    prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"

    if print_prompt:
        print("-" * 90)
        print(prompt)
        print("-" * 90)
    
    response = model.generate_content(prompt)
    return response

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response_with_faiss(
                prompt, df, embeddings, model, embed_text, time, local_choice
            )
            placeholder = st.empty()
            full_response = response if isinstance(response, str) else response.text
            placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
